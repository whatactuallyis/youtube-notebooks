{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cq2eaz4qz7t"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install kaggle package quietly\n",
        "!pip install -q kaggle==1.5.12\n",
        "\n",
        "# Create a kaggle folder\n",
        "!mkdir ~/.kaggle\n",
        "# Copy json file into the kaggle folder\n",
        "!cp /content/gdrive/MyDrive/recording/imdb-reviews/kaggle.json ~/.kaggle/\n",
        "# Give full read & write permission only to the owner\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "# Download the IMDb Reviews dataset\n",
        "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "# Unzip the downloaded dataset\n",
        "!unzip imdb-dataset-of-50k-movie-reviews.zip\n",
        "# Delete the zip file\n",
        "!rm -rf imdb-dataset-of-50k-movie-reviews.zip\n",
        "\n",
        "# Unmount the gdrive\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "-d6J3MierCrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the modules\n",
        "import pandas as pd\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ],
      "metadata": {
        "id": "PVDTnXlf49VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "df = pd.read_csv(filepath_or_buffer=\"/content/IMDB Dataset.csv\")\n",
        "# Trace\n",
        "df.sample(n=5, random_state=42)"
      ],
      "metadata": {
        "id": "GL577_YVskkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape and the memory usage of the data\n",
        "print(f\"Shape: {df.shape}, Memory: {round(df.memory_usage(deep=True).sum() * 1e-6, 3)} MB.\")"
      ],
      "metadata": {
        "id": "nqvTaFJIuxzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_missing(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Returns the missing percentages of the given frame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The Dataframe. \n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The missing percentages.\n",
        "    \"\"\"\n",
        "    # Calculate the number of missing values\n",
        "    missing = df.isnull().sum()\n",
        "    # Calculate the missing percentages\n",
        "    percent_missing = missing * 100 / len(df)\n",
        "    # Create a frame with dict format\n",
        "    missing_value_df = pd.DataFrame(\n",
        "        {\"num_missing\": missing, \"percent_missing\": percent_missing}\n",
        "    )\n",
        "    # Round the percentage values\n",
        "    missing_value_df = round(number=missing_value_df, ndigits=2)\n",
        "    # Sort from highest to lowest\n",
        "    missing_value_df.sort_values(\"percent_missing\", inplace=True)\n",
        "    # Return the missing value frame\n",
        "    return missing_value_df\n",
        "\n",
        "\n",
        "# Check the missing value portions of the data\n",
        "df_missing = calc_missing(df=df); df_missing"
      ],
      "metadata": {
        "id": "pFN1ChcwwC8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the modules\n",
        "from functools import partial\n",
        "from re import sub\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "gM_CJ-C-wHpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download missing nltk packages\n",
        "download('punkt')\n",
        "download('wordnet')\n",
        "download('omw-1.4')\n",
        "download('stopwords')"
      ],
      "metadata": {
        "id": "VoirMuonpg-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wnl = WordNetLemmatizer()\n",
        "swords = stopwords.words(\"english\")\n",
        "\n",
        "\n",
        "def process_text(wnl: WordNetLemmatizer, swords: list, text: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Process the given text by applying;\n",
        "        - HTML parsing,\n",
        "        - RegEx ops for brackets and non-alphanumeric chars in the words,\n",
        "        - Tokenization,\n",
        "        - Lowercase,\n",
        "        - Lemmatization,\n",
        "        - Removing stop words,\n",
        "        - Removing more than one space.\n",
        "\n",
        "    Args:\n",
        "        wnl (WordNetLemmatizer): Lemmatizer.\n",
        "        swords (list): Stop words list.\n",
        "        text (pd.Series): Text to be processed.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: Processed text.\n",
        "    \"\"\"\n",
        "    # Parse the HTML\n",
        "    text = BeautifulSoup(markup=text, features=\"html.parser\").get_text()\n",
        "    # Remove the brackets\n",
        "    text = sub(pattern=r\"[\\[\\]\\(\\)\\{\\}]\", repl=\" \", string=text)\n",
        "    # Remove the numbers from the text\n",
        "    text = sub(pattern=r\"[^a-zA-z0-9\\s]\", repl=\" \", string=text)\n",
        "    # Tokenize the text\n",
        "    text = word_tokenize(text=text)\n",
        "    # Lowercase\n",
        "    text = [t.lower() for t in text]\n",
        "    # Lemmatize the text\n",
        "    text = [wnl.lemmatize(t) for t in text]\n",
        "    # Remove the stop words from the processed text\n",
        "    text = [t for t in text if t not in swords]\n",
        "    # Join back the tokens\n",
        "    text = \" \".join(text)\n",
        "    # Remove more than one space due to processing ops\n",
        "    text = sub(pattern=r\"\\s+\", repl=\" \", string=text)\n",
        "    # Return the processed text\n",
        "    return text\n",
        "\n",
        "df.insert(loc=1,\n",
        "          column=\"processed_review\",\n",
        "          value=df.review.apply(func=partial(process_text, wnl, swords))\n",
        "        )\n",
        "df"
      ],
      "metadata": {
        "id": "om9EAqQ6K_eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Memory: {round(df.drop(columns=['review'], axis=1).memory_usage(deep=True).sum() * 1e-6, 3)} MB.\")"
      ],
      "metadata": {
        "id": "ABVHUxxYZUmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Hero Approach"
      ],
      "metadata": {
        "id": "gf57ruyqhrxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -yq nltk\n",
        "!pip uninstall -yq spacy\n",
        "\n",
        "!pip install -Uq texthero"
      ],
      "metadata": {
        "id": "N0rKsT5qhuz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm # To solve https://github.com/jbesomi/texthero/issues/122\n",
        "\n",
        "# Import the modules\n",
        "import texthero as hero\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "U_2i_W0pdtxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload the data\n",
        "df_hero = pd.read_csv(filepath_or_buffer=\"/content/IMDB Dataset.csv\")"
      ],
      "metadata": {
        "id": "yWEO4N2Cx-7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing pipeline by Texthero\n",
        "df_hero.loc[:, \"processed_review\"] = df_hero.loc[:, \"review\"]\\\n",
        "    .pipe(hero.remove_html_tags)\\\n",
        "    .pipe(hero.remove_urls)\\\n",
        "    .pipe(hero.remove_brackets)\\\n",
        "    .pipe(hero.clean)\n",
        "# Trace\n",
        "df_hero.sample(n=5, random_state=42)"
      ],
      "metadata": {
        "id": "pGSald-uyP5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the top number of words by the sentiment\n",
        "df_hero.groupby(\"sentiment\")[\"processed_review\"].apply(\n",
        "    lambda x: hero.top_words(x)[:10]\n",
        "    )"
      ],
      "metadata": {
        "id": "nKHJpIyn0pYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[:5, :]"
      ],
      "metadata": {
        "id": "UOeyuwXV7k4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tfidf\n",
        "df_hero.loc[:, \"tfidf\"] = df_hero.loc[:, \"processed_review\"]\\\n",
        "    .pipe(hero.tfidf, max_features=1000)\n",
        "# Trace\n",
        "df_hero.loc[:, \"pca\"] = df_hero.loc[:, \"tfidf\"].pipe(hero.pca)"
      ],
      "metadata": {
        "id": "A7ZDheXRnsgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hero.loc[0, \"tfidf\"][:]"
      ],
      "metadata": {
        "id": "debBZHoptyfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot visualization\n",
        "hero.scatterplot(df=df_hero,\n",
        "                 col=\"pca\",\n",
        "                 color=\"sentiment\",\n",
        "                 title=\"PCA IMDb Reviews\")"
      ],
      "metadata": {
        "id": "ZSzoFENNpUxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-means with 3 clusters\n",
        "df_hero.loc[:, \"topics\"] = df_hero.loc[:, \"tfidf\"].pipe(hero.kmeans, n_clusters=3)\n",
        "hero.scatterplot(df=df_hero,\n",
        "                 col=\"pca\",\n",
        "                 color=\"topics\",\n",
        "                 title=\"PCA IMDb Reviews by 3 topics\")"
      ],
      "metadata": {
        "id": "8kRH6HTDp_Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Positive wordcloud\n",
        "hero.visualization.wordcloud(s=df_hero.loc[df_hero.sentiment == \"positive\", \"processed_review\"],\n",
        "                             max_words=20)"
      ],
      "metadata": {
        "id": "S6qwdl7Sq4BN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}