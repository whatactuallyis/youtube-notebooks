{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCRY790TwTe5",
        "outputId": "c9c7e6a1-b92e-49a1-e109-3f8c74e17e5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.8/719.8 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Fastbook + relevant packages installation\n",
        "! [ -e /content ] && pip install -Uqq fastbook kaggle\n",
        "\n",
        "# Import the modules\n",
        "import fastbook\n",
        "# Setup the book\n",
        "fastbook.setup_book()\n",
        "\n",
        "# Import the modules\n",
        "from fastai.tabular.all import *\n",
        "from fastbook import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a kaggle folder\n",
        "!mkdir ~/.kaggle\n",
        "# Copy json file into the kaggle folder\n",
        "!cp /content/gdrive/MyDrive/YouTube/kaggle.json ~/.kaggle/\n",
        "# Give full read & write permission only to the owner\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "0wowKeBeyYMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare the dataset name\n",
        "name = \"titanic\"\n",
        "# Declare the dataset path\n",
        "path = URLs.path(url=name); path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGOBiTHbygEe",
        "outputId": "7ed7a7a9-9f62-425b-b9af-953ce5a90f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Path('/root/.fastai/archive/titanic')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the modules\n",
        "from kaggle import api\n",
        "\n",
        "# If the path does not exist\n",
        "if not path.exists():\n",
        "    # Create it\n",
        "    path.mkdir(parents=true)\n",
        "    # Download the dataset to the set path\n",
        "    api.competition_download_cli(competition=name, path=path)\n",
        "    # Unzip the file\n",
        "    shutil.unpack_archive(str(path/f\"{name}.zip\"), str(path))\n",
        "\n",
        "# Trace\n",
        "path.ls(file_type=\"text\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTW56CikyxbF",
        "outputId": "ae8302f0-fec8-4b4f-ab60-fb191b7781a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading titanic.zip to /root/.fastai/archive/titanic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34.1k/34.1k [00:00<00:00, 2.26MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#3) [Path('/root/.fastai/archive/titanic/train.csv'),Path('/root/.fastai/archive/titanic/gender_submission.csv'),Path('/root/.fastai/archive/titanic/test.csv')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the modules\n",
        "from pickle import load\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/YouTube/mLPlaylist/titanic-fastai-to.pkl\", \"rb\") as f:\n",
        "    # Load the TabularPandas object [Titanic Dataset]\n",
        "    to = load(file=f)\n",
        "\n",
        "# Trace\n",
        "to.show(max_n=5, random_state=43)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "mLN6xdA-yy7u",
        "outputId": "41c550db-3d93-4282-c698-c99ac4f65979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sex</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Embarked</th>\n",
              "      <th>Deck</th>\n",
              "      <th>Title</th>\n",
              "      <th>Age_na</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Alone</th>\n",
              "      <th>Family</th>\n",
              "      <th>Survived</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>847</th>\n",
              "      <td>male</td>\n",
              "      <td>3</td>\n",
              "      <td>C</td>\n",
              "      <td>#na#</td>\n",
              "      <td>Mr</td>\n",
              "      <td>False</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>823</th>\n",
              "      <td>female</td>\n",
              "      <td>3</td>\n",
              "      <td>S</td>\n",
              "      <td>DE</td>\n",
              "      <td>Mrs</td>\n",
              "      <td>False</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>775</th>\n",
              "      <td>male</td>\n",
              "      <td>3</td>\n",
              "      <td>S</td>\n",
              "      <td>#na#</td>\n",
              "      <td>Mr</td>\n",
              "      <td>False</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>female</td>\n",
              "      <td>3</td>\n",
              "      <td>Q</td>\n",
              "      <td>#na#</td>\n",
              "      <td>Miss</td>\n",
              "      <td>True</td>\n",
              "      <td>29.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>male</td>\n",
              "      <td>3</td>\n",
              "      <td>C</td>\n",
              "      <td>#na#</td>\n",
              "      <td>Mr</td>\n",
              "      <td>True</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To get the data for machine learning approaches\n",
        "# outside the fastai solutions\n",
        "X_train, y_train = to.train.xs, to.train.ys.values.ravel() # Training\n",
        "X_val, y_val = to.valid.xs, to.valid.ys.values.ravel() # Validation"
      ],
      "metadata": {
        "id": "f0VuDJPAGoS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning"
      ],
      "metadata": {
        "id": "nJ0ngYL9IHkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import modules\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Declare the model\n",
        "rfc = RandomForestClassifier(max_depth=7,\n",
        "                            min_samples_split=3,\n",
        "                            oob_score=True,\n",
        "                            random_state=43,\n",
        "                            verbose=1)\n",
        "# Fit the model\n",
        "rfc.fit(X=X_train, y=y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "et0wFgt6Gz06",
        "outputId": "88bed518-8cd0-44a6-a2a0-e532ed58c0f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.6s finished\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(max_depth=7, min_samples_split=3, oob_score=True,\n",
              "                       random_state=43, verbose=1)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=7, min_samples_split=3, oob_score=True,\n",
              "                       random_state=43, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=7, min_samples_split=3, oob_score=True,\n",
              "                       random_state=43, verbose=1)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFQ5a00AR-5x"
      },
      "source": [
        "# Neptune + Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "header",
          "comment"
        ],
        "id": "GZWxXNwPR-50"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "comment"
        ],
        "id": "BqkCYT5KR-50"
      },
      "source": [
        "This guide will show you how to:\n",
        "\n",
        "* Install `neptune-client`,\n",
        "* Connect Neptune to your Colab notebook and create the first run,\n",
        "* Log simple metrics to Neptune and explore them in the UI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHNIt11AR-51"
      },
      "source": [
        "## Before you start\n",
        "\n",
        "Make sure that:\n",
        "* you have a [Google account](https://support.google.com/accounts/answer/27441?hl=en)\n",
        "* If you want to see the example recorded to your own workspace instead:\n",
        "    * Create a Neptune account → [Take me to registration](https://neptune.ai/register)\n",
        "    * Create a Neptune project that you will use for tracking metadata → [Tell me more about projects](https://docs.neptune.ai/administration/projects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "header",
          "installation"
        ],
        "id": "9vVk3axoR-52"
      },
      "source": [
        "## Install Neptune and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "installation"
        ],
        "id": "Cd9prCd7R-52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd2a16d4-1ee9-4ffc-9232-5b1a2391eb39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 KB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.7/134.7 KB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.5/135.5 KB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q neptune-client\n",
        "!pip install -Uqq neptune-sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFQZJbCvR-54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45fe98d6-7295-40fb-e5fd-8a99247892c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/neptune/internal/backends/hosted_client.py:48: NeptuneDeprecationWarning: The 'neptune-client' package has been deprecated and will be removed in the future. Install the 'neptune' package instead. For more, see https://docs.neptune.ai/setup/upgrading/\n",
            "  from neptune.version import version as neptune_client_version\n",
            "<ipython-input-9-37c521bab1e7>:1: NeptuneDeprecationWarning: You're importing the Neptune client library via the deprecated `neptune.new` module, which will be removed in a future release. Import directly from `neptune` instead.\n",
            "  import neptune.new as neptune\n"
          ]
        }
      ],
      "source": [
        "import neptune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "header"
        ],
        "id": "N8jpWkCDR-54"
      },
      "source": [
        "## Initialize Neptune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "comment"
        ],
        "id": "3qqPGwsRR-55"
      },
      "source": [
        "Connect your script to Neptune and create a new run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "comment"
        ],
        "id": "IMji2kjpR-56"
      },
      "source": [
        "Basically, you tell Neptune: \n",
        "\n",
        "* **who you are**: your Neptune `api_token` \n",
        "* **where you want to send your data**: your Neptune `project`.\n",
        "\n",
        "At this point, you will have a new run in Neptune. From now on you will use `run` to log metadata to it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "header",
          "exclude"
        ],
        "id": "4dvrI-efR-56"
      },
      "source": [
        "### Get your personal `api_token` to initialize Neptune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXN7Ytq9R-57"
      },
      "source": [
        "**Note:**<br>\n",
        "There are a few special, public projects to show how Neptune works. For those projects, you can use the 'ANONYMOUS' api token and log as a public user `neptuner`.  \n",
        "\n",
        "For example:  \n",
        "```python\n",
        "run = neptune.init_run(api_token=neptune.ANONYMOUS_API_TOKEN,\n",
        "                   project=\"common/neptune-and-google-colab\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "comment",
          "exclude"
        ],
        "id": "wStyeMWGR-57"
      },
      "source": [
        "Get your [Neptune API token](https://docs.neptune.ai/getting-started/installation#authentication) and pass it to Neptune:\n",
        "![get_token.gif](https://neptune.ai/wp-content/uploads/get_token-2.gif)\n",
        "\n",
        "The preferred way of doing this is by using the `getpass()` method so that your token remains private even if you share the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "code",
          "exclude"
        ],
        "id": "1sMWLdRcR-57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e81dc50e-42b3-4f86-8e00-f15fc004ab81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your private Neptune API token: ··········\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "api_token = getpass(\"Enter your private Neptune API token: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzjsk1iLR-58"
      },
      "source": [
        "You can log as an anonymous user `neptuner` with `api_token=neptune.ANONYMOUS_API_TOKEN`  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "header",
          "exclude"
        ],
        "id": "gzstnZ9pR-58"
      },
      "source": [
        "### Initialize your project\n",
        "\n",
        "Remember to [create a new project](https://docs.neptune.ai/administration/workspace-project-and-user-management/projects#create-project) from the UI that you will use for metadata tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "code",
          "exclude"
        ],
        "id": "yDjkFfIGR-58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deef6f68-2f4b-4955-c248-fa979c72a880"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wai/example\n"
          ]
        }
      ],
      "source": [
        "workspace = \"wai\"\n",
        "project_name = \"example\"\n",
        "project = f\"{workspace}/{project_name}\"\n",
        "\n",
        "print(project)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vJegb_ER-58"
      },
      "source": [
        "**Tip:** The `project_name` of a project can be found under its Settings → Properties\n",
        "![project name](https://neptune.ai/wp-content/uploads/project-name-neptune.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-diqWNcR-59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c0101d-da34-417a-b495-0af45882a8b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://app.neptune.ai/wai/example/e/EX-1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:neptune.internal.hardware.gpu.gpu_monitor:Info (NVML): Driver Not Loaded. GPU usage metrics may not be reported. For more information, see https://docs.neptune.ai/help/nvml_error/\n"
          ]
        }
      ],
      "source": [
        "run = neptune.init_run(project=project,\n",
        "                       api_token=api_token,\n",
        "                       capture_hardware_metrics=True,\n",
        "                       capture_stderr=True,\n",
        "                       capture_stdout=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "comment",
          "exclude"
        ],
        "id": "X9Ulu4IzR-59"
      },
      "source": [
        "Click on the link above to open this run in Neptune. For now it is empty but keep the tab with run open to see what happens next. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "comment"
        ],
        "id": "Ermzojq4R-59"
      },
      "source": [
        "Runs can be viewed as dictionary-like structures - **namespaces** - that you can define in your code. You can apply a hierarchical structure to your metadata that will be reflected in the UI as well. Thanks to this you can easily organize your metadata in a way you feel is most convenient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "header"
        ],
        "id": "XlQVLC8lR-59"
      },
      "source": [
        "## Log metadata during training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "comment"
        ],
        "id": "c-EBLp8tR-59"
      },
      "source": [
        "Log metrics or losses under a name of your choice. You can log one or multiple values.\n",
        "\n",
        "Now run the cell below, and switch over to the Neptune app to view the live logging."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the modules\n",
        "import neptune.new.integrations.sklearn as npt_utils\n",
        "from neptune.utils import stringify_unsupported\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "tWNK7jM69OYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "cr = classification_report(y_true=y_val,\n",
        "                           y_pred=rfc.predict(X=X_val),\n",
        "                           output_dict=True)\n",
        "# Trace\n",
        "cr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3g40y_GLmm9",
        "outputId": "89fbc6fd-4a84-4191-a5b4-4dbc03805119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0': {'precision': 0.7989130434782609,\n",
              "  'recall': 0.9245283018867925,\n",
              "  'f1-score': 0.8571428571428572,\n",
              "  'support': 159},\n",
              " '1': {'precision': 0.8554216867469879,\n",
              "  'recall': 0.6574074074074074,\n",
              "  'f1-score': 0.7434554973821988,\n",
              "  'support': 108},\n",
              " 'accuracy': 0.8164794007490637,\n",
              " 'macro avg': {'precision': 0.8271673651126243,\n",
              "  'recall': 0.7909678546471,\n",
              "  'f1-score': 0.8002991772625281,\n",
              "  'support': 267},\n",
              " 'weighted avg': {'precision': 0.821770472216173,\n",
              "  'recall': 0.8164794007490637,\n",
              "  'f1-score': 0.8111569588127033,\n",
              "  'support': 267}}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve metric results\n",
        "accuracy = cr[\"accuracy\"]\n",
        "precision = cr[\"macro avg\"][\"precision\"]\n",
        "recall = cr[\"macro avg\"][\"recall\"]\n",
        "f1 = cr[\"macro avg\"][\"f1-score\"]"
      ],
      "metadata": {
        "id": "8RF89ISDPxa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "code"
        ],
        "id": "YvbMgJfwR-5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1c3c410-1964-4e76-f8a0-0a43dc60ca56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
          ]
        }
      ],
      "source": [
        "# log params\n",
        "run[\"parameters\"] = stringify_unsupported(npt_utils.get_estimator_params(estimator=rfc))\n",
        "\n",
        "# log name and append tags\n",
        "run[\"sys/name\"] = \"randomforest-neptune-example\"\n",
        "run[\"sys/tags\"].add([\"randomforest\", \"colab\"])\n",
        "\n",
        "# # Evaluation metrics\n",
        "# run[\"scores/accuracy\"] = accuracy\n",
        "# run[\"scores/precision\"] = precision\n",
        "# run[\"scores/recall\"] = recall\n",
        "# run[\"scores/f1\"] = f1\n",
        "\n",
        "# Classifier summary\n",
        "run[\"rfc_summary\"] = npt_utils.create_classifier_summary(classifier=rfc,\n",
        "                                                         X_train=X_train,\n",
        "                                                         X_test=X_val,\n",
        "                                                         y_train=y_train,\n",
        "                                                         y_test=y_val)\n",
        "# Confusion matrix\n",
        "run[\"confusion-matrix\"] = npt_utils.create_confusion_matrix_chart(classifier=rfc,\n",
        "                                                                  X_train=X_train,\n",
        "                                                                  X_test=X_val,\n",
        "                                                                  y_train=y_train,\n",
        "                                                                  y_test=y_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the modules\n",
        "from neptune.types import File\n",
        "\n",
        "run[\"model/pickled_model\"].upload(File.as_pickle(obj=rfc))"
      ],
      "metadata": {
        "id": "OWVQrHIhSKIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzVRC1_xR-5-"
      },
      "source": [
        "The snippet above logs:\n",
        "\n",
        "* `parameters` with just one field: learning rate,\n",
        "* name of run and two tags,\n",
        "* `train/loss` and `train/loss-pow-2` as series of numbers, visualized as charts in UI,\n",
        "* `train/accuracy` and `valid/accuracy` as single values\n",
        "* `file.txt` which will be visible under All Metadata/artifacts as sample.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKNeBVgsR-5-"
      },
      "source": [
        "**Tip:**<br>\n",
        "To view the structure of a run, use the `print_structure()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0y8mOV1NR-5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b13eb345-8cdb-4d3f-df9d-39107e4d6aff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94m'confusion-matrix'\u001b[0m: File\n",
            "\u001b[94m'model'\u001b[0m:\n",
            "    \u001b[94m'pickled_model'\u001b[0m: File\n",
            "\u001b[94m'monitoring'\u001b[0m:\n",
            "    \u001b[94m'61b94d2a'\u001b[0m:\n",
            "        \u001b[94m'cpu'\u001b[0m: FloatSeries\n",
            "        \u001b[94m'hostname'\u001b[0m: String\n",
            "        \u001b[94m'memory'\u001b[0m: FloatSeries\n",
            "        \u001b[94m'pid'\u001b[0m: String\n",
            "        \u001b[94m'stderr'\u001b[0m: StringSeries\n",
            "        \u001b[94m'stdout'\u001b[0m: StringSeries\n",
            "        \u001b[94m'tid'\u001b[0m: String\n",
            "\u001b[94m'parameters'\u001b[0m:\n",
            "    \u001b[94m'bootstrap'\u001b[0m: Boolean\n",
            "    \u001b[94m'ccp_alpha'\u001b[0m: Float\n",
            "    \u001b[94m'class_weight'\u001b[0m: String\n",
            "    \u001b[94m'criterion'\u001b[0m: String\n",
            "    \u001b[94m'max_depth'\u001b[0m: Integer\n",
            "    \u001b[94m'max_features'\u001b[0m: String\n",
            "    \u001b[94m'max_leaf_nodes'\u001b[0m: String\n",
            "    \u001b[94m'max_samples'\u001b[0m: String\n",
            "    \u001b[94m'min_impurity_decrease'\u001b[0m: Float\n",
            "    \u001b[94m'min_samples_leaf'\u001b[0m: Integer\n",
            "    \u001b[94m'min_samples_split'\u001b[0m: Integer\n",
            "    \u001b[94m'min_weight_fraction_leaf'\u001b[0m: Float\n",
            "    \u001b[94m'n_estimators'\u001b[0m: Integer\n",
            "    \u001b[94m'n_jobs'\u001b[0m: String\n",
            "    \u001b[94m'oob_score'\u001b[0m: Boolean\n",
            "    \u001b[94m'random_state'\u001b[0m: Integer\n",
            "    \u001b[94m'verbose'\u001b[0m: Integer\n",
            "    \u001b[94m'warm_start'\u001b[0m: Boolean\n",
            "\u001b[94m'rfc_summary'\u001b[0m:\n",
            "    \u001b[94m'all_params'\u001b[0m:\n",
            "        \u001b[94m'bootstrap'\u001b[0m: Boolean\n",
            "        \u001b[94m'ccp_alpha'\u001b[0m: Float\n",
            "        \u001b[94m'class_weight'\u001b[0m: String\n",
            "        \u001b[94m'criterion'\u001b[0m: String\n",
            "        \u001b[94m'max_depth'\u001b[0m: Integer\n",
            "        \u001b[94m'max_features'\u001b[0m: String\n",
            "        \u001b[94m'max_leaf_nodes'\u001b[0m: String\n",
            "        \u001b[94m'max_samples'\u001b[0m: String\n",
            "        \u001b[94m'min_impurity_decrease'\u001b[0m: Float\n",
            "        \u001b[94m'min_samples_leaf'\u001b[0m: Integer\n",
            "        \u001b[94m'min_samples_split'\u001b[0m: Integer\n",
            "        \u001b[94m'min_weight_fraction_leaf'\u001b[0m: Float\n",
            "        \u001b[94m'n_estimators'\u001b[0m: Integer\n",
            "        \u001b[94m'n_jobs'\u001b[0m: String\n",
            "        \u001b[94m'oob_score'\u001b[0m: Boolean\n",
            "        \u001b[94m'random_state'\u001b[0m: Integer\n",
            "        \u001b[94m'verbose'\u001b[0m: Integer\n",
            "        \u001b[94m'warm_start'\u001b[0m: Boolean\n",
            "    \u001b[94m'diagnostics_charts'\u001b[0m:\n",
            "        \u001b[94m'ROC_AUC'\u001b[0m: File\n",
            "        \u001b[94m'class_prediction_error'\u001b[0m: File\n",
            "        \u001b[94m'classification_report'\u001b[0m: File\n",
            "        \u001b[94m'confusion_matrix'\u001b[0m: File\n",
            "        \u001b[94m'precision_recall'\u001b[0m: File\n",
            "    \u001b[94m'integration'\u001b[0m:\n",
            "        \u001b[94m'about'\u001b[0m:\n",
            "            \u001b[94m'neptune-sklearn'\u001b[0m: String\n",
            "    \u001b[94m'pickled_model'\u001b[0m: File\n",
            "    \u001b[94m'test'\u001b[0m:\n",
            "        \u001b[94m'preds'\u001b[0m: File\n",
            "        \u001b[94m'preds_proba'\u001b[0m: File\n",
            "        \u001b[94m'scores'\u001b[0m:\n",
            "            \u001b[94m'class_0'\u001b[0m:\n",
            "                \u001b[94m'fbeta_score'\u001b[0m: Float\n",
            "                \u001b[94m'precision'\u001b[0m: Float\n",
            "                \u001b[94m'recall'\u001b[0m: Float\n",
            "                \u001b[94m'support'\u001b[0m: Float\n",
            "            \u001b[94m'class_1'\u001b[0m:\n",
            "                \u001b[94m'fbeta_score'\u001b[0m: Float\n",
            "                \u001b[94m'precision'\u001b[0m: Float\n",
            "                \u001b[94m'recall'\u001b[0m: Float\n",
            "                \u001b[94m'support'\u001b[0m: Float\n",
            "\u001b[94m'scores'\u001b[0m:\n",
            "    \u001b[94m'accuracy'\u001b[0m: Float\n",
            "    \u001b[94m'f1'\u001b[0m: Float\n",
            "    \u001b[94m'precision'\u001b[0m: Float\n",
            "    \u001b[94m'recall'\u001b[0m: Float\n",
            "\u001b[94m'sys'\u001b[0m:\n",
            "    \u001b[94m'creation_time'\u001b[0m: Datetime\n",
            "    \u001b[94m'description'\u001b[0m: String\n",
            "    \u001b[94m'failed'\u001b[0m: Boolean\n",
            "    \u001b[94m'hostname'\u001b[0m: String\n",
            "    \u001b[94m'id'\u001b[0m: String\n",
            "    \u001b[94m'modification_time'\u001b[0m: Datetime\n",
            "    \u001b[94m'monitoring_time'\u001b[0m: Integer\n",
            "    \u001b[94m'name'\u001b[0m: String\n",
            "    \u001b[94m'owner'\u001b[0m: String\n",
            "    \u001b[94m'ping_time'\u001b[0m: Datetime\n",
            "    \u001b[94m'running_time'\u001b[0m: Float\n",
            "    \u001b[94m'size'\u001b[0m: Float\n",
            "    \u001b[94m'state'\u001b[0m: RunState\n",
            "    \u001b[94m'tags'\u001b[0m: StringSet\n",
            "    \u001b[94m'trashed'\u001b[0m: Boolean\n"
          ]
        }
      ],
      "source": [
        "run.print_structure()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lJ7XNrnR-5_"
      },
      "source": [
        "## Stop logging  \n",
        "<font color=red>**Warning:**</font><br>\n",
        "Once you are done logging, you should stop tracking the run using the `stop()` method.\n",
        "This is needed only while logging from a notebook environment. While logging through a script, Neptune automatically stops tracking once the script has completed execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cASufHNIR-5_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01bb8434-bc99-4851-ff55-380c5c6c5362"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shutting down background jobs, please wait a moment...\n",
            "Done!\n",
            "Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n",
            "All 1 operations synced, thanks for waiting!\n",
            "Explore the metadata in the Neptune app:\n",
            "https://app.neptune.ai/wai/example/e/EX-1/metadata\n"
          ]
        }
      ],
      "source": [
        "run.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "comment"
        ],
        "id": "KOnhDjtxR-5_"
      },
      "source": [
        "## Explore the run in the Neptune app\n",
        "\n",
        "Go to the `All metadata` and `Charts` sections of the Neptune app to see them. You can also check an [example run](https://app.neptune.ai/o/common/org/showroom/e/SHOW-37/charts) \n",
        "\n",
        "![colab_example_charts.png](https://neptune.ai/wp-content/uploads/charts-in-neptune-2.png)\n",
        "\n",
        "**Tip:**\n",
        "\n",
        "Neptune automatically logs the hardware consumption during the run. \n",
        "\n",
        "You can see it in the `Monitoring` section of the Neptune app. \n",
        "\n",
        "![image](https://neptune.ai/wp-content/uploads/Product_hardware-usage-1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "comment"
        ],
        "id": "FoZ8rpnTR-5_"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "You’ve learned how to:\n",
        "* Install `neptune-client`,\n",
        "* Connect Neptune to your Google Colab notebook and create a run,\n",
        "* Log metadata to Neptune,\n",
        "* See your metrics parameters and scores,\n",
        "* See hardware consumption during the run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "comment"
        ],
        "id": "tP_lsjGxR-5_"
      },
      "source": [
        "## What's next\n",
        "\n",
        "Now that you know how to create runs and log metrics, you can learn:\n",
        "\n",
        "* [How to log other types of metadata to Neptune](https://docs.neptune.ai/you-should-know/logging-and-managing-runs-results/logging-runs-data#what-objects-can-you-log-to-neptune)\n",
        "* [How to download runs data from Neptune](https://docs.neptune.ai/user-guides/logging-and-managing-runs-results/downloading-runs-data)\n",
        "* [How to connect Neptune to the ML framework you are using](https://docs.neptune.ai/essentials/integrations)"
      ]
    }
  ]
}